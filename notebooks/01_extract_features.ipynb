{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24a48add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55e73aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"D:\\speech_emotion\\speech-emotion-classification\\data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccdf06d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files found: 2880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D:\\\\speech_emotion\\\\speech-emotion-classification\\\\data\\\\Actor_01\\\\03-01-01-01-01-01-01.wav',\n",
       " 'D:\\\\speech_emotion\\\\speech-emotion-classification\\\\data\\\\Actor_01\\\\03-01-01-01-01-02-01.wav',\n",
       " 'D:\\\\speech_emotion\\\\speech-emotion-classification\\\\data\\\\Actor_01\\\\03-01-01-01-02-01-01.wav',\n",
       " 'D:\\\\speech_emotion\\\\speech-emotion-classification\\\\data\\\\Actor_01\\\\03-01-01-01-02-02-01.wav',\n",
       " 'D:\\\\speech_emotion\\\\speech-emotion-classification\\\\data\\\\Actor_01\\\\03-01-02-01-01-01-01.wav']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "for root, dirs, f in os.walk(data_path):\n",
    "    for file in f:\n",
    "        if file.endswith(\".wav\"):\n",
    "            files.append(os.path.join(root, file))\n",
    "\n",
    "print(\"Total audio files found:\", len(files))\n",
    "files[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adf79dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "    mfcc_mean = np.mean(mfcc.T, axis=0)\n",
    "\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    chroma_mean = np.mean(chroma.T, axis=0)\n",
    "\n",
    "    centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    centroid_mean = np.mean(centroid)\n",
    "\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "    zcr_mean = np.mean(zcr)\n",
    "\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    rolloff_mean = np.mean(rolloff)\n",
    "\n",
    "    return np.hstack([mfcc_mean, chroma_mean, centroid_mean, zcr_mean, rolloff_mean])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25868f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_from_path(path):\n",
    "    file = os.path.basename(path)\n",
    "    emotion_code = int(file.split(\"-\")[2])\n",
    "\n",
    "    emotion_map = {\n",
    "        1: \"neutral\",\n",
    "        2: \"calm\",\n",
    "        3: \"happy\",\n",
    "        4: \"sad\",\n",
    "        5: \"angry\",\n",
    "        6: \"fear\",\n",
    "        7: \"disgust\",\n",
    "        8: \"surprise\"\n",
    "    }\n",
    "\n",
    "    return emotion_map[emotion_code]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa289212",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        features = extract_features(file)\n",
    "        emotion = get_emotion_from_path(file)\n",
    "        data.append([file, emotion] + list(features))\n",
    "    except Exception as e:\n",
    "        print(\"Error processing:\", file, \"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4f33962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>emotion</th>\n",
       "      <th>mfcc_0</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>...</th>\n",
       "      <th>chroma_5</th>\n",
       "      <th>chroma_6</th>\n",
       "      <th>chroma_7</th>\n",
       "      <th>chroma_8</th>\n",
       "      <th>chroma_9</th>\n",
       "      <th>chroma_10</th>\n",
       "      <th>chroma_11</th>\n",
       "      <th>centroid</th>\n",
       "      <th>zcr</th>\n",
       "      <th>rolloff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\speech_emotion\\speech-emotion-classificatio...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-726.217224</td>\n",
       "      <td>68.541420</td>\n",
       "      <td>3.293398</td>\n",
       "      <td>12.205300</td>\n",
       "      <td>5.510278</td>\n",
       "      <td>13.667410</td>\n",
       "      <td>-2.983828</td>\n",
       "      <td>3.098029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617627</td>\n",
       "      <td>0.630300</td>\n",
       "      <td>0.641174</td>\n",
       "      <td>0.643986</td>\n",
       "      <td>0.623945</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.629672</td>\n",
       "      <td>7416.297748</td>\n",
       "      <td>0.050476</td>\n",
       "      <td>13285.735887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\speech_emotion\\speech-emotion-classificatio...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-719.128296</td>\n",
       "      <td>70.201569</td>\n",
       "      <td>1.168397</td>\n",
       "      <td>13.122541</td>\n",
       "      <td>7.836950</td>\n",
       "      <td>14.411290</td>\n",
       "      <td>-4.111360</td>\n",
       "      <td>4.468973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629304</td>\n",
       "      <td>0.598250</td>\n",
       "      <td>0.602427</td>\n",
       "      <td>0.638104</td>\n",
       "      <td>0.650110</td>\n",
       "      <td>0.663277</td>\n",
       "      <td>0.638956</td>\n",
       "      <td>7135.571471</td>\n",
       "      <td>0.052904</td>\n",
       "      <td>13191.643371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\speech_emotion\\speech-emotion-classificatio...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-714.995728</td>\n",
       "      <td>69.689346</td>\n",
       "      <td>3.924564</td>\n",
       "      <td>11.924190</td>\n",
       "      <td>6.421723</td>\n",
       "      <td>11.011614</td>\n",
       "      <td>-2.878103</td>\n",
       "      <td>4.509558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595411</td>\n",
       "      <td>0.606914</td>\n",
       "      <td>0.611433</td>\n",
       "      <td>0.634772</td>\n",
       "      <td>0.586808</td>\n",
       "      <td>0.578905</td>\n",
       "      <td>0.612411</td>\n",
       "      <td>7239.265648</td>\n",
       "      <td>0.046627</td>\n",
       "      <td>13279.137826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\speech_emotion\\speech-emotion-classificatio...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-710.975281</td>\n",
       "      <td>67.564880</td>\n",
       "      <td>5.782240</td>\n",
       "      <td>13.230727</td>\n",
       "      <td>6.190846</td>\n",
       "      <td>12.628252</td>\n",
       "      <td>-1.675169</td>\n",
       "      <td>5.657494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646860</td>\n",
       "      <td>0.619975</td>\n",
       "      <td>0.611885</td>\n",
       "      <td>0.633945</td>\n",
       "      <td>0.597960</td>\n",
       "      <td>0.602110</td>\n",
       "      <td>0.619935</td>\n",
       "      <td>7008.958169</td>\n",
       "      <td>0.053835</td>\n",
       "      <td>13272.074245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\speech_emotion\\speech-emotion-classificatio...</td>\n",
       "      <td>calm</td>\n",
       "      <td>-759.921753</td>\n",
       "      <td>75.783524</td>\n",
       "      <td>6.023605</td>\n",
       "      <td>14.557394</td>\n",
       "      <td>6.454188</td>\n",
       "      <td>14.631508</td>\n",
       "      <td>-3.004551</td>\n",
       "      <td>4.620970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620046</td>\n",
       "      <td>0.594329</td>\n",
       "      <td>0.596532</td>\n",
       "      <td>0.616511</td>\n",
       "      <td>0.638365</td>\n",
       "      <td>0.670592</td>\n",
       "      <td>0.602803</td>\n",
       "      <td>6997.311810</td>\n",
       "      <td>0.045929</td>\n",
       "      <td>12649.543486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  emotion      mfcc_0  \\\n",
       "0  D:\\speech_emotion\\speech-emotion-classificatio...  neutral -726.217224   \n",
       "1  D:\\speech_emotion\\speech-emotion-classificatio...  neutral -719.128296   \n",
       "2  D:\\speech_emotion\\speech-emotion-classificatio...  neutral -714.995728   \n",
       "3  D:\\speech_emotion\\speech-emotion-classificatio...  neutral -710.975281   \n",
       "4  D:\\speech_emotion\\speech-emotion-classificatio...     calm -759.921753   \n",
       "\n",
       "      mfcc_1    mfcc_2     mfcc_3    mfcc_4     mfcc_5    mfcc_6    mfcc_7  \\\n",
       "0  68.541420  3.293398  12.205300  5.510278  13.667410 -2.983828  3.098029   \n",
       "1  70.201569  1.168397  13.122541  7.836950  14.411290 -4.111360  4.468973   \n",
       "2  69.689346  3.924564  11.924190  6.421723  11.011614 -2.878103  4.509558   \n",
       "3  67.564880  5.782240  13.230727  6.190846  12.628252 -1.675169  5.657494   \n",
       "4  75.783524  6.023605  14.557394  6.454188  14.631508 -3.004551  4.620970   \n",
       "\n",
       "   ...  chroma_5  chroma_6  chroma_7  chroma_8  chroma_9  chroma_10  \\\n",
       "0  ...  0.617627  0.630300  0.641174  0.643986  0.623945   0.633900   \n",
       "1  ...  0.629304  0.598250  0.602427  0.638104  0.650110   0.663277   \n",
       "2  ...  0.595411  0.606914  0.611433  0.634772  0.586808   0.578905   \n",
       "3  ...  0.646860  0.619975  0.611885  0.633945  0.597960   0.602110   \n",
       "4  ...  0.620046  0.594329  0.596532  0.616511  0.638365   0.670592   \n",
       "\n",
       "   chroma_11     centroid       zcr       rolloff  \n",
       "0   0.629672  7416.297748  0.050476  13285.735887  \n",
       "1   0.638956  7135.571471  0.052904  13191.643371  \n",
       "2   0.612411  7239.265648  0.046627  13279.137826  \n",
       "3   0.619935  7008.958169  0.053835  13272.074245  \n",
       "4   0.602803  6997.311810  0.045929  12649.543486  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = (\n",
    "    [\"file\", \"emotion\"] +\n",
    "    [f\"mfcc_{i}\" for i in range(40)] +\n",
    "    [f\"chroma_{i}\" for i in range(12)] +\n",
    "    [\"centroid\", \"zcr\", \"rolloff\"]\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa0e6ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(r\"D:\\speech_emotion\\speech-emotion-classification\\features\\ravdess_features.csv\", index=False)\n",
    "print(\"Features saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
